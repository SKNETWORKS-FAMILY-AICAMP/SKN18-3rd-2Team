"""
ì½˜ì†” RAG: LangChain ìž„ë² ë”© + ê¸°ì¡´ SQL ê²€ìƒ‰ + LangChain PromptTemplate ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±.
"""
import os
from typing import List, Sequence, Tuple

from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser

from common import search_similar

RowType = Tuple[float, int, str, str, str, str]


PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë‹¹ì‹ ì€ ì •í™•í•œ í•œêµ­ì–´ ë©´ì ‘ ì½”ì¹˜ìž…ë‹ˆë‹¤. ì œê³µëœ ê·¼ê±°ë¥¼ í™œìš©í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\n"
            "[ê·¼ê±° Q/A]\n{context}",
        ),
        ("human", "{question}"),
    ]
)


def build_prompt_inputs(rows: Sequence[RowType], user_query: str) -> dict[str, str]:
    """
    ê²€ìƒ‰ëœ Q/A í–‰ë“¤ì„ LangChain PromptTemplateì— ë°”ë¡œ ë„£ì„ ìˆ˜ ìžˆëŠ” dictë¡œ ë³€í™˜í•œë‹¤.
    - top_n: í™˜ê²½ë³€ìˆ˜ RAG_MAX_ITEMS (ê¸°ë³¸ 5)
    - ê° í•­ëª©ì€ "#ë²ˆí˜¸ (ëŒ€ë¶„ë¥˜/ì¤‘ë¶„ë¥˜) Q: ... A: ..." í¬ë§·
    - ë„ˆë¬´ ê¸´ í•­ëª©ì€ RAG_MAX_CHARS ê¸°ì¤€ìœ¼ë¡œ ìž˜ë¼ë‚¸ë‹¤.
    """
    top_n = int(os.getenv("RAG_MAX_ITEMS", "5"))
    max_chars = int(os.getenv("RAG_MAX_CHARS", "1200"))

    formatted_blocks: List[str] = []
    for rank, (_score, _id, big, mid, question, answer) in enumerate(rows[:top_n], start=1):
        block = f"#{rank} ({big}/{mid})\nQ: {question}\nA: {answer}"
        if len(block) > max_chars:
            block = block[:max_chars] + " â€¦(ìƒëžµ)"
        formatted_blocks.append(block)

    context_text = "\n\n".join(formatted_blocks)
    question_text = (
        f"ì§ˆë¬¸: {user_query}\n\n"
        "ìš”ì²­:\n"
        "- ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìž‘ì„±\n"
        "- ì¶”ì¸¡ì€ í”¼í•˜ê³  ê·¼ê±°ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ë‚´ìš©ë§Œ ì‚¬ìš©\n"
        "- ë‹µë³€ ëì— ì°¸ê³  ê·¼ê±° ë²ˆí˜¸ë¥¼ 'ì°¸ê³ : #1, #2' í˜•íƒœë¡œ ëª…ì‹œ"
    )
    return {"context": context_text, "question": question_text}


def build_chain():
    temperature = float(os.getenv("GEN_TEMPERATURE", "0.2"))
    model_name = os.getenv("OLLAMA_MODEL")
    base_url = os.getenv("OLLAMA_HOST")
    llm = ChatOllama(model=model_name, temperature=temperature, base_url=base_url)
    return PROMPT_TEMPLATE | llm | StrOutputParser()


def chat_loop():
    chain = build_chain()
    print("ðŸ’¬ ë©´ì ‘ Q/A ì±—ë´‡ìž…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'bye' ì¤‘ í•˜ë‚˜ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
    while True:
        try:
            user_query = input("\nì§ˆë¬¸> ").strip()
        except EOFError:
            print("\nìž…ë ¥ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if not user_query:
            print("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ê±°ë‚˜ ì¢…ë£Œ ëª…ë ¹ì„ ìž…ë ¥í•˜ì„¸ìš”.")
            continue
        if user_query.lower() in {"quit", "exit", "bye"}:
            print("ì•ˆë…•ížˆ ê°€ì„¸ìš”!")
            break

        rows = search_similar(user_query, k=8)
        if not rows:
            print("â— ê´€ë ¨ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°”ê¿”ë³´ì„¸ìš”.")
            continue

        prompt_inputs = build_prompt_inputs(rows, user_query)
        answer = chain.invoke(prompt_inputs)

        print("\n=== ë‹µë³€ ===\n")
        print(answer)
        print("\n--- ì°¸ê³ í•œ Q/A ---")
        for idx, (score, _id, big, mid, question, _answer) in enumerate(rows[:5], start=1):
            preview = question[:80] + ("..." if len(question) > 80 else "")
            print(f"#{idx} [{score:.3f}] ({big}/{mid}) Q: {preview}")


def main():
    load_dotenv()
    chat_loop()


if __name__ == "__main__":
    main()
