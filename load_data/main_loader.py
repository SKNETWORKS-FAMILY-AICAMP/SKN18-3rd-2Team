"""
ë©”ì¸ ë°ì´í„° ë¡œë” - ì•½í’ˆ ì •ë³´ JSON â†’ VectorDB
"""

from tqdm import tqdm
from config import (
    connect_db, 
    load_json_documents, 
    get_text_splitter,
    get_embedding_model, 
    create_embedding
)

def insert_data(json_path=None, clear_mode=False):
    """JSONì„ VectorDBì— ì‚½ì…"""
    
    # ê²½ë¡œ ìë™ ì„¤ì •
    if json_path is None:
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        json_path = os.path.join(current_dir, "..", "data", "drug_info_preprocessed.json")
    
    # ì—°ê²° ë° ì¤€ë¹„
    conn = connect_db()
    print("âœ… DB ì—°ê²° ì™„ë£Œ")
    
    # JSON ë¬¸ì„œ ë¡œë“œ
    documents = load_json_documents(json_path)
    print(f"âœ… JSON ë¡œë“œ: {len(documents)}ê°œ ì•½í’ˆ")
    
    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì¤€ë¹„
    splitter = get_text_splitter()
    print("âœ… í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì¤€ë¹„")
    
    # ë¬¸ì„œ ì²­í‚¹ ë¨¼ì € ìˆ˜í–‰
    print("\nğŸ“„ ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
    all_chunks = []
    
    for doc in documents:
        # ê° ì•½í’ˆ ì •ë³´ë¥¼ ì²­í‚¹
        chunks = splitter.split_documents([doc])
        all_chunks.extend(chunks)
    
    print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬")
    
    # ì„ë² ë”© í”Œë«í¼ ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ì˜ˆ: HuggingFace, OpenAI ë“±)
    model, platform_name = get_embedding_model(return_platform_name=True) if 'return_platform_name' in get_embedding_model.__code__.co_varnames else (get_embedding_model(), "Unknown Platform")
    print(f"\nğŸ¤– {platform_name} ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    cursor = conn.cursor()
    
    try:
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (clear_modeê°€ Trueì¼ ë•Œ)
        if clear_mode:
            print("ğŸ—‘ï¸  ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘...")
            cursor.execute("DELETE FROM qa_embedding")
            conn.commit()
            print("âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        
        # VectorDBì— ì„ë² ë”© ì‚½ì…
        print("\nğŸ”® VectorDB ì„ë² ë”© ì‚½ì… ì¤‘...")
        inserted_count = 0
        
        for chunk in tqdm(all_chunks):
            # ì²­í¬ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ìƒì„±
            embedding = create_embedding(chunk.page_content, model)
            
            # ì œí’ˆëª…ì„ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ
            product_name = chunk.metadata.get('ì œí’ˆëª…', 'Unknown')
            
            # qa_embedding í…Œì´ë¸”ì— ì‚½ì… (RDB í…Œì´ë¸” ì—†ì´ VectorDBë§Œ ì‚¬ìš©)
            import psycopg2.extras
            metadata_dict = {'ì œí’ˆëª…': product_name, 'content': chunk.page_content}
            cursor.execute("""
                INSERT INTO qa_embedding (embedding, metadata)
                VALUES (%s, %s)
            """, (embedding, psycopg2.extras.Json(metadata_dict)))
            
            inserted_count += 1
        
        conn.commit()
        print(f"âœ… ì„ë² ë”© ì‚½ì… ì™„ë£Œ: {inserted_count}ê°œ")
        
        # ê²€ì¦
        cursor.execute("SELECT COUNT(*) FROM qa_embedding")
        embedding_count = cursor.fetchone()[0]
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"- ì„ë² ë”©: {embedding_count}ê°œ")
        print("ğŸ‰ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

if __name__ == "__main__":
    insert_data()