"""
í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ê¸°ëŠ¥
Hugging Face ëª¨ë¸ ì‚¬ìš© (ë¡œì»¬ ì‹¤í–‰)
E5 ëª¨ë¸ ìµœì í™” í¬í•¨
"""
import os
from typing import List
from sentence_transformers import SentenceTransformer
import numpy as np
from dotenv import load_dotenv

load_dotenv()

class EmbeddingGenerator:
    def __init__(self, model_name: str = None):
        # ëª¨ë¸ ì´ë¦„ ê²°ì •
        model_key = model_name or os.getenv('EMBEDDING_MODEL', 'e5_large_instruct')
        
        # ì¶”ì²œ ëª¨ë¸ í‚¤ì›Œë“œì¸ì§€ í™•ì¸
        if model_key in RECOMMENDED_MODELS:
            self.model_name = RECOMMENDED_MODELS[model_key]
        else:
            self.model_name = model_key  # ì§ì ‘ ëª¨ë¸ëª…ì´ ì…ë ¥ëœ ê²½ìš°
        
        # E5 ëª¨ë¸ì¸ì§€ í™•ì¸ (ë” ì •í™•í•œ ê°ì§€)
        self.is_e5_model = ('e5' in self.model_name.lower() or 
                           'multilingual-e5' in self.model_name.lower())
        
        print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        if self.is_e5_model:
            print("ğŸ“‹ E5 ëª¨ë¸ ê°ì§€ - íŠ¹ë³„í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì‚¬ìš©")
            print("   - ë¬¸ì„œ: 'passage: <text>'")
            print("   - ì¿¼ë¦¬: 'query: <text>'")
        
        try:
            self.model = SentenceTransformer(self.model_name)
            embedding_dim = self.model.get_sentence_embedding_dimension()
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì„ë² ë”© ì°¨ì›: {embedding_dim}")
            
            # E5 ëª¨ë¸ì˜ ê²½ìš° 1024ì°¨ì›ì¸ì§€ í™•ì¸
            if self.is_e5_model and embedding_dim != 1024:
                print(f"âš ï¸ ì£¼ì˜: E5 ëª¨ë¸ì´ì§€ë§Œ ì„ë² ë”© ì°¨ì›ì´ {embedding_dim}ì…ë‹ˆë‹¤. 1024ì°¨ì›ì´ ì˜ˆìƒë©ë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        return self.model.get_sentence_embedding_dimension()
    
    def generate_embedding(self, text: str, is_query: bool = False) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸°)
            max_length = 512  # ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ìµœëŒ€ ê¸¸ì´
            if len(text) > max_length * 4:  # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
                text = text[:max_length * 4]
            
            # E5 ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì‚¬ìš©
            if self.is_e5_model:
                if is_query:
                    # ì§ˆë¬¸/ê²€ìƒ‰ ì¿¼ë¦¬ìš© í”„ë¡¬í”„íŠ¸
                    formatted_text = f"query: {text.strip()}"
                else:
                    # ë¬¸ì„œ/íŒ¨ì‹œì§€ìš© í”„ë¡¬í”„íŠ¸
                    formatted_text = f"passage: {text.strip()}"
            else:
                formatted_text = text.strip()
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.model.encode(
                formatted_text, 
                convert_to_tensor=False,
                normalize_embeddings=True  # ì •ê·œí™”ë¡œ ì„±ëŠ¥ í–¥ìƒ
            )
            
            # numpy arrayë¥¼ listë¡œ ë³€í™˜
            if isinstance(embedding, np.ndarray):
                return embedding.tolist()
            else:
                return embedding
                
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
            print(f"   í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}...")
            raise
    
    def generate_embeddings_batch(self, texts: List[str], is_query: bool = False, batch_size: int = 4) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆì— ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            max_length = 512 * 4
            processed_texts = []
            
            for text in texts:
                if len(text) > max_length:
                    text = text[:max_length]
                processed_texts.append(text.strip())
            
            # E5 ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì‚¬ìš©
            if self.is_e5_model:
                if is_query:
                    formatted_texts = [f"query: {text}" for text in processed_texts]
                else:
                    formatted_texts = [f"passage: {text}" for text in processed_texts]
            else:
                formatted_texts = processed_texts
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
            all_embeddings = []
            for i in range(0, len(formatted_texts), batch_size):
                batch = formatted_texts[i:i + batch_size]
                batch_embeddings = self.model.encode(
                    batch, 
                    convert_to_tensor=False,
                    normalize_embeddings=True,
                    batch_size=min(batch_size, len(batch))
                )
                
                # numpy arrayë¥¼ listë¡œ ë³€í™˜
                if isinstance(batch_embeddings, np.ndarray):
                    all_embeddings.extend([emb.tolist() for emb in batch_embeddings])
                else:
                    all_embeddings.extend(batch_embeddings)
            
            return all_embeddings
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"   ë°°ì¹˜ í¬ê¸°: {len(texts)}")
            raise

# ì¶”ì²œ ëª¨ë¸ë“¤ (ì†ë„ vs ì„±ëŠ¥)
RECOMMENDED_MODELS = {
    "fast": "sentence-transformers/all-MiniLM-L6-v2",  # 384ì°¨ì›, ë§¤ìš° ë¹ ë¦„
    "multilingual_small": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # 384ì°¨ì›, ë¹ ë¦„
    "multilingual_medium": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",  # 768ì°¨ì›, ê· í˜•
    "korean_optimized": "jhgan/ko-sroberta-multitask",  # 768ì°¨ì›, í•œêµ­ì–´ íŠ¹í™”
    "e5_base": "intfloat/multilingual-e5-base",  # 768ì°¨ì›, ë¹ ë¥¸ E5
    "e5_large": "intfloat/multilingual-e5-large",  # 1024ì°¨ì›, ì„±ëŠ¥ ì¢‹ìŒ
    "e5_large_instruct": "intfloat/multilingual-e5-large-instruct",  # 1024ì°¨ì›, ìµœê³  ì„±ëŠ¥ (ëŠë¦¼)
}