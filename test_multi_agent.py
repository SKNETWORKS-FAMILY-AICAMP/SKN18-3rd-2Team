"""
Multi-Agent RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰ (ë¡œì»¬ LLM ì‚¬ìš©)
"""
from src.multi_agent_system import MultiAgentCoordinator

import requests
import json

class EnhancedMultiAgentRAG:
    """ë¡œì»¬ LLMê³¼ ì—°ë™ëœ í–¥ìƒëœ Multi-Agent RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, use_local_llm: bool = False, llm_model: str = "gemma3:4b"):
        self.coordinator = MultiAgentCoordinator()
        self.use_local_llm = use_local_llm
        self.llm_model = llm_model
        self.local_llm = None
        self.ollama_url = "http://localhost:11434/api/generate"
        self.ollama_base_url = "http://localhost:11434"
        
        print(f"ğŸš€ Enhanced Multi-Agent RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ‘¥ í™œì„± ì—ì´ì „íŠ¸: {len(self.coordinator.agents)}ê°œ")
        print(f"ğŸ³ Docker Ollama ëª¨ë“œ: {llm_model}")
        
        # Docker Ollama ì»¨í…Œì´ë„ˆ ì‚¬ìš©
    
    def query_with_llm_synthesis(self, query: str, user_context: dict = None) -> dict:
        """Multi-agent ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±"""
        
        print(f"ğŸ” ì¿¼ë¦¬: {query}")
        
        # 1. Multi-agent ì‹œìŠ¤í…œìœ¼ë¡œ ì •ë³´ ìˆ˜ì§‘
        agent_results = self.coordinator.process_query(query, user_context)
        
        if "error" in agent_results:
            return {"error": agent_results["error"]}
        
        # 2. LLMì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._create_synthesis_prompt(query, agent_results)
        
        # 3. LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        llm_response = self._call_llm(prompt)
        
        return {
            "final_answer": llm_response,
            "agent_analysis": agent_results,
            "query": query,
            "model_used": self.llm_model
        }
    
    def _create_synthesis_prompt(self, query: str, agent_results: dict) -> str:
        """Multi-agent ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        detailed_responses = agent_results.get("detailed_responses", [])
        sources = agent_results.get("all_sources", [])
        
        prompt = f"""ë‹¹ì‹ ì€ ì˜ì•½í’ˆ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤ì´ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì „ë¬¸ ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼:
"""
        
        for response in detailed_responses:
            prompt += f"""
[{response['agent']}] (ì‹ ë¢°ë„: {response['confidence']:.2f})
- ë¶„ì„: {response['reasoning']}
- ê²°ê³¼: {response['response']}
"""
        
        if sources:
            prompt += f"\nì°¸ì¡° ì˜ì•½í’ˆ: {', '.join(sources[:5])}"
        
        prompt += """

ë‹µë³€ ì§€ì¹¨:
1. ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤ì˜ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
2. ì˜í•™ì  ì¡°ì–¸ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì˜ì•½í’ˆ ì •ë³´ì„ì„ ëª…ì‹œí•˜ì„¸ìš”
3. êµ¬ì²´ì ì¸ ì˜ì•½í’ˆëª…ê³¼ ì£¼ìš” ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œí•˜ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”
5. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”

ì¢…í•© ë‹µë³€:"""
        
        return prompt
    
    def _call_llm(self, prompt: str) -> str:
        """gemma3:4b ëª¨ë¸ë¡œ LLM í˜¸ì¶œ (Ollama ì „ìš©)"""
        try:
            response = requests.post(
                self.ollama_url,
                json={
                    "model": "gemma3:4b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.9,
                        "max_tokens": 800
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            else:
                return f"LLM ì„œë²„ ì˜¤ë¥˜: {response.status_code}"
                
        except requests.exceptions.ConnectionError:
            return "âŒ Docker Ollama ì»¨í…Œì´ë„ˆì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'docker-compose up -d' ëª…ë ¹ìœ¼ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”."
        except Exception as e:
            return f"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}"
    
    def interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print("\nğŸ’¬ Multi-Agent RAG ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘!")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("-" * 50)
        
        while True:
            try:
                query = input("\nì§ˆë¬¸: ").strip()
                
                if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not query:
                    continue
                
                print("\nğŸ”„ ë¶„ì„ ì¤‘...")
                result = self.query_with_llm_synthesis(query)
                
                if "error" in result:
                    print(f"âŒ ì˜¤ë¥˜: {result['error']}")
                    continue
                
                print(f"\nğŸ¤– ë‹µë³€:")
                print(result["final_answer"])
                
                # ì—ì´ì „íŠ¸ ë¶„ì„ ì •ë³´ (ì„ íƒì  í‘œì‹œ)
                show_details = input("\nìƒì„¸ ë¶„ì„ ì •ë³´ë¥¼ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() == 'y'
                if show_details:
                    self._show_agent_details(result["agent_analysis"])
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _show_agent_details(self, agent_analysis: dict):
        """ì—ì´ì „íŠ¸ ë¶„ì„ ìƒì„¸ ì •ë³´ í‘œì‹œ"""
        print("\nğŸ“Š ì—ì´ì „íŠ¸ ë¶„ì„ ìƒì„¸:")
        print("-" * 30)
        
        for response in agent_analysis.get("detailed_responses", []):
            print(f"\nğŸ¤– {response['agent']} (ì‹ ë¢°ë„: {response['confidence']:.2f})")
            print(f"   ë¶„ì„: {response['reasoning']}")
            print(f"   ê²°ê³¼: {response['response']}")
        
        sources = agent_analysis.get("all_sources", [])
        if sources:
            print(f"\nğŸ“š ì°¸ì¡° ì˜ì•½í’ˆ: {', '.join(sources)}")

def check_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    print("ğŸ” ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘...")
    
    # 1. ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
    try:
        from src.database import VectorDB
        db = VectorDB()
        cursor = db.connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM documents;")
        doc_count = cursor.fetchone()[0]
        db.close()
        
        print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤: {doc_count}ê°œ ë¬¸ì„œ ì €ì¥ë¨")
        
        if doc_count < 100:
            print("âš ï¸ ë¬¸ì„œ ìˆ˜ê°€ ì ìŠµë‹ˆë‹¤. ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ë°ì´í„° ë¡œë”©ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
        return False
    
    # 2. Ollama ë¡œì»¬ ì„œë²„ ë° gemma3:4b ëª¨ë¸ í™•ì¸
    try:
        print("ğŸ”— Ollama ë¡œì»¬ ì„œë²„ ì—°ê²° ì¤‘...")
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m['name'] for m in models]
            
            print(f"ğŸ“‹ ë¡œì»¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models)}ê°œ")
            
            if 'gemma3:4b' in model_names:
                print("âœ… Ollama ë¡œì»¬ ì„œë²„: gemma3:4b ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
                
                # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                for model in models:
                    if model['name'] == 'gemma3:4b':
                        size = model.get('size', 'Unknown')
                        print(f"ğŸ“Š gemma3:4b ëª¨ë¸ í¬ê¸°: {size}")
                        break
            else:
                print("âš ï¸ gemma3:4b ëª¨ë¸ì´ ë¡œì»¬ì— ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
                print("   ollama pull gemma3:4b")
                return False
                
        else:
            print(f"âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ Docker Ollama ì»¨í…Œì´ë„ˆì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("   1. docker-compose up -d (Docker ì„œë¹„ìŠ¤ ì‹œì‘)")
        print("   2. python setup_docker_ollama.py (ìë™ ì„¤ì •)")
        return False
    except Exception as e:
        print(f"âŒ Ollama ì—°ê²° ì˜¤ë¥˜: {e}")
        return False
    
    return True

if __name__ == "__main__":
    print("ğŸš€ Enhanced Multi-Agent RAG ì‹œìŠ¤í…œ (gemma3:4b)")
    print("=" * 50)
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    if not check_system_status():
        print("\nâŒ ì‹œìŠ¤í…œ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("   1. docker-compose up -d")
        print("   2. python setup_docker_ollama.py")
        exit(1)
    
    # Multi-Agent RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (gemma3:4b ê³ ì •)
    enhanced_rag = EnhancedMultiAgentRAG(use_local_llm=False, llm_model="gemma3:4b")
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    print("\nì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ëŒ€í™”í˜• ëª¨ë“œ")
    print("2. í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    
    choice = input("ì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if choice == "1":
        enhanced_rag.interactive_mode()
    else:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        test_queries = [
            "ë‘í†µì— ì¢‹ì€ ì•½ì´ ìˆë‚˜ìš”?",
            "ê²Œë³´ë¦°ì •ì˜ ìš©ë²•ê³¼ ìš©ëŸ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì„ì‹  ì¤‘ì— í”¼í•´ì•¼ í•  ì•½ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰")
        for i, query in enumerate(test_queries, 1):
            print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {query}")
            result = enhanced_rag.query_with_llm_synthesis(query)
            print(f"ë‹µë³€: {result.get('final_answer', 'No response')}")
            print("-" * 50)